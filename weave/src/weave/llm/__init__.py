"""
LLM integration layer.

Provides abstraction over local LLM inference using llama-cpp-python:
- Model loading and configuration
- Streaming token generation
- Chat completion formatting
- Context window management
"""

