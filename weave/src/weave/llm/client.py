"""
llama-cpp-python wrapper.

Handles model loading with appropriate settings:
- Quantization level configuration
- GPU layer offloading (n_gpu_layers)
- Context window size (n_ctx)
- Memory management

Provides a clean interface for the rest of the application.
"""

